{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yctrqVhquaMo"
      },
      "source": [
        "#Machine learning Project\n",
        "\n",
        "#Student : Tariq\n",
        "\n",
        "#Teacher : Felix\n",
        "\n",
        "#instructor: Hannas\n",
        "\n",
        "\n",
        "#Question i am trying to answer:\n",
        "\n",
        "Can I use the features such as Animal Age, Animal Gender, Animal Name, Animal Breed, and Animal Type to predict whether an animal will be adopted?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hcbgd00OWn8m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from pandas import DataFrame, Series\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pandas import DataFrame, Series\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from datetime import datetime\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from pprint import pprint\n",
        "from wordcloud import WordCloud\n",
        "from IPython.display import Image\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLnjlMddYih-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "0437c7ad-acb9-4a3c-f64c-30cc05984894"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-014e7881d4c5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OutcomeType'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OutcomeType'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'Adoption'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Color\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Color'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "df['OutcomeType']=df['OutcomeType']=='Adoption'\n",
        "df[\"Color\"]= df['Color'].replace('/', ' ', regex=True)\n",
        "\n",
        "\n",
        "#######################################################################\n",
        "df=df.drop('DateTime',axis=1)    ##Features i have been asked to remove\n",
        "df=df.drop('OutcomeSubtype',axis=1)\n",
        "df=df.drop('AnimalID',axis=1)\n",
        "#######################################################################\n",
        "\n",
        "\n",
        "X_train, X_test =train_test_split(df, test_size=0.35,random_state=42)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3_E7v-3vgIC"
      },
      "source": [
        "We had Five classes i took only Adoption\n",
        "\n",
        "\n",
        "Reasons:\n",
        "I found it impractical to predict classes when I have an even smaller number of features available.\n",
        "#I have been asked to drop  the feature\n",
        "\n",
        "\n",
        "\n",
        "DataTime\n",
        "OutCome subtype\n",
        "Animal Id                    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for different reasons :: Mainly to get reduce the    |accuarcy score|\n",
        "\n",
        "\n",
        "Here, we are dividing the data to ensure maximum separation for effective feature handling.without manipulating the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFztRkJJcc9w"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7, 5))\n",
        "_ = sns.countplot(data=X_train, x='OutcomeType', palette='Set3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SIX3YdzxcRb"
      },
      "source": [
        "#Implanced Dataset:\n",
        "\n",
        "In the full dataset, there were originally 26,000 samples. For the purposes of our analysis, we have randomly selected a subset for X_train, which we are using to create plots.\n",
        "\n",
        "One key observation is that the number of animals that have not been adopted significantly outweighs the number of animals that have found homes. This dataset exhibits class imbalance, where one class (not adopted) is much more prevalent than the other (adopted).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAfWKQ5UyBnQ"
      },
      "source": [
        "***Feature handeling  :***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTSrCX1_fsmj"
      },
      "outputs": [],
      "source": [
        "def Animal_is_Named(DataFrame):\n",
        "  DataFrame[\"Name\"]=DataFrame['Name'].fillna('NoName')\n",
        "  DataFrame[\"Name\"]=DataFrame[\"Name\"]!='NoName'\n",
        "\n",
        "\n",
        "Animal_is_Named(X_train)\n",
        "Animal_is_Named(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNt4LeLGyKp3"
      },
      "source": [
        "This part of the process checks whether an animal has a name or not. It does so by filling any missing values with \"NoName.\" Afterward, it evaluates whether the animal's name is \"NoName.\" If it's \"False,\" it means there is no a name, and if it's \"True,\" it indicates that the animal is named."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu28cLcVNmhd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6dZTOEpYiJU"
      },
      "outputs": [],
      "source": [
        "counts = X_train.groupby('Name')['OutcomeType'].value_counts(normalize=True).mul(100).unstack()\n",
        "\n",
        "counts.plot.barh(stacked=True, colormap='Set1', figsize=(10, 2))\n",
        "plt.xlabel('Percentage')\n",
        "plt.ylabel('Name (Yes/No)')\n",
        "plt.title('The Relation Between having a name and Adoption')\n",
        "plt.tick_params(axis='y', which='major', labelsize=14, labelrotation=0, length=5)\n",
        "plt.show();\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlkpYbmOssQb"
      },
      "outputs": [],
      "source": [
        "counts = X_train.groupby('AnimalType')['OutcomeType'].value_counts(normalize=True).mul(100).unstack()\n",
        "\n",
        "counts.plot.barh(stacked=True, colormap='Set1', figsize=(10, 2))\n",
        "plt.xlabel('Percentage')\n",
        "plt.ylabel('Cat Vs Dog')\n",
        "plt.title('The Relation Between having a name and Adoption')\n",
        "plt.tick_params(axis='y', which='major', labelsize=14, labelrotation=0, length=5)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsIQHprEn8Ja"
      },
      "outputs": [],
      "source": [
        "X_train[\"AnimalType\"]=X_train['AnimalType']=='Cat'\n",
        "X_test[\"AnimalType\"]=X_test['AnimalTyphow to plot a confusiuon matrix resembling the correlation between the outcome and the e']=='Cat'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbNA0g8sy8Os"
      },
      "source": [
        "\n",
        "In simple terms, to check if an animal is a cat, we just look at its \"animal type.\" If it says \"cat,\" it's true (it's a cat); if it says anything else, it's false (it's not a cat, it's a dog)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w74zbO2YMnRm"
      },
      "outputs": [],
      "source": [
        "def AnimalType_Named(DataFrame):\n",
        "  Condition=(DataFrame[\"AnimalType\"]==False)&(DataFrame[\"Name\"]==True)\n",
        "  DataFrame[\"Named_Dog\"]=(DataFrame[\"AnimalType\"]==False)&(DataFrame[\"Name\"]==True)\n",
        "  DataFrame[\"Named_Cat\"]=(DataFrame[\"AnimalType\"]==True)&(DataFrame[\"Name\"]==True)\n",
        "\n",
        "AnimalType_Named(X_train)\n",
        "AnimalType_Named(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9xTGIIEZbXI"
      },
      "outputs": [],
      "source": [
        "counts = X_train.groupby('SexuponOutcome')['OutcomeType'].value_counts(normalize=True).mul(100).unstack()\n",
        "\n",
        "counts.plot.barh(stacked=True, colormap='Set1', figsize=(10, 3))  # figsize here has effect\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Gender')\n",
        "plt.title('Gender Contribution')\n",
        "plt.tick_params(axis='y', which='major', labelsize=14, labelrotation=10, length=5)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoTXl6z9dQ3r"
      },
      "outputs": [],
      "source": [
        "\n",
        "def Gender_converting(Data_Frame):\n",
        "  Data_Frame['Male'] = Data_Frame['SexuponOutcome'].str.contains('Male').astype(bool)\n",
        "  Data_Frame['intact']=Data_Frame['SexuponOutcome'].str.contains(\"Intact\").astype(bool)\n",
        "  Data_Frame['Unknown']=Data_Frame['SexuponOutcome'].str.contains(\"Unknown\").astype(bool)\n",
        "  Data_Frame['Spayed']=Data_Frame['SexuponOutcome'].str.contains(\"Spayed\").astype(bool)\n",
        "  Data_Frame['Neutered']=Data_Frame['SexuponOutcome'].str.contains(\"Neutered\").astype(bool)\n",
        "  return Data_Frame\n",
        "\n",
        "def Gender_converting(Data_Frame):\n",
        "  Data_Frame['Male'] = Data_Frame['SexuponOutcome'].str.contains('Male').astype(bool)\n",
        "  Data_Frame['intact'\n",
        "X_train=Gender_converting(X_train)\n",
        "X_test=Gender_converting(X_test)\n",
        "\n",
        "\n",
        "\n",
        "X_train=X_train.drop(\"SexuponOutcome\",axis=True)\n",
        "X_test=X_test.drop(\"SexuponOutcome\",axis=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBNwV8rj5OyV"
      },
      "source": [
        "The gender have been divided into five features:\n",
        "\n",
        "Male , Intact , Spayed , Neutered , Unknown\n",
        "\n",
        "It have been handeled by looking if the sample satisfy the string of the feature\n",
        "\n",
        "Example:\n",
        "Neutered Male We have  in the String Male\n",
        "Then Male : 1 and we have neutered : 1 And everything else is going to be zero\n",
        "\n",
        "Male : 1 , Intact : 0 , Spayed : 0 ,Neutered : 1 ,Unknown : 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT0X48LSM37T"
      },
      "outputs": [],
      "source": [
        "value_counts = df[\"Color\"].value_counts()[:20]\n",
        "value_counts.plot(kind='bar', figsize=(10, 6))\n",
        "plt.xlabel(\"Color\")\n",
        "plt.ylabel('Frequency')\n",
        "plt.title(f'Histogram of {\"Common Colors\"}')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNj-14ePEzWg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def Common_Colors(DataFrame):\n",
        "  common_breeds = X_train[\"Color\"].value_counts()[40:]\n",
        "  common_breeds = common_breeds.index.tolist()\n",
        "  X_train['Color'] = X_train['Color'].replace(common_breeds, 'others')\n",
        "  X_test['Color'] = X_test['Color'].replace(common_breeds, 'others')\n",
        "\n",
        "Common_Colors(X_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_unique_colors(data_frame):\n",
        "    data_frame[\"Color\"] = data_frame['Color'].replace('/', ' ', regex=True)\n",
        "    color_list = data_frame[\"Color\"].str.split().explode()\n",
        "    unique_colors = np.unique(color_list)\n",
        "    return list(unique_colors)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cKCGEfYwA3q"
      },
      "outputs": [],
      "source": [
        "\n",
        "def Color_Converting(DataFrame,train_Set):\n",
        "\n",
        "  Possible_Colors=extract_unique_colors(train_Set)\n",
        "\n",
        "  DataFrame[\"Color\"]=DataFrame['Color'].replace('/', ' ', regex=True)\n",
        "\n",
        "  Color_row=list()\n",
        "  for i in DataFrame['Color']:\n",
        "    i = np.array(i.split(' '))\n",
        "    indices = np.where(np.isin(Possible_Colors,i))\n",
        "    Color_row.append(indices)\n",
        "\n",
        "\n",
        "  Matrix=np.zeros((len(DataFrame[\"Color\"]),len(Possible_Colors)))\n",
        "\n",
        "  for i in range(len(DataFrame[\"Color\"])):\n",
        "    Matrix[i][Color_row[i]]=1\n",
        "\n",
        "  Matrix=Matrix.T\n",
        "\n",
        "\n",
        "  for Counter, Colors in enumerate(Possible_Colors):\n",
        "    DataFrame[Colors]=Matrix[Counter]\n",
        "\n",
        "Color_Converting(X_train,X_train)\n",
        "Color_Converting(X_test,X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxo5AexxTS4_"
      },
      "outputs": [],
      "source": [
        "X_train=X_train.drop(\"Color\",axis=True)\n",
        "X_test=X_test.drop(\"Color\",axis=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvwRxQNNDhbC"
      },
      "source": [
        "Three hundred colors were present; as a result, not all of them were included. Instead, the fifty most common colors were considered, and everything else was transferred into the \"other\" category.\n",
        "Yet, due to the large number of colors, they were transformed into the one-hot encoding format.  \n",
        "       Example :\n",
        "set of colors :Black, White , Red\n",
        "A black and white dog is considered.\n",
        "Result: Black - 1, White - 1, Red - 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmNpPGyyicFh"
      },
      "source": [
        "in terms of the explanation the color feature was one of the hardest features that have been used yet\n",
        "\n",
        "\n",
        "the summary is that :\n",
        "\n",
        "\n",
        "An array have been defined in order to capture color in a way that :\n",
        "\n",
        "\n",
        "\n",
        "A=[‘Black’,’White’,’Red’]\n",
        "\n",
        "\n",
        "each color represents the column that it will be presenting the Color in the matrix\n",
        "\n",
        "\n",
        "Then the color of the animal have been looked in that array such that the index where the color is will be where the matrix should be transferred into One and everything else will be zero\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Summary : We are trying to make one hot encoding in order to present the colors in a simple way for the machine learning module to understand\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHBGGh6EHX0u"
      },
      "outputs": [],
      "source": [
        "value_counts = df[\"Breed\"].value_counts()[:20]\n",
        "value_counts.plot(kind='bar', figsize=(10, 6))\n",
        "plt.xlabel(\"breed\")\n",
        "plt.ylabel('Frequency')\n",
        "plt.title(f'Histogram of {\"breed\"}')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "excXCmEKkCIM"
      },
      "source": [
        "The Breed Feature have been transformed into the OnehotEncoding and this in order to clear up the realtions between the feature and the Outcome the we are trying to predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYKAIJAelo0u"
      },
      "source": [
        "\n",
        "#The Breed Feature\n",
        "\n",
        "So, basically, I have two types of dogs in my dataset: Boxer Dogs and Pitbull Dogs. Instead of using one-hot encoding, I have created two separate features:\n",
        "\n",
        "\"I have a \"Pitbull\" feature: This feature is set to \"true\" if the dog is a Pitbull and \"false\" if it's not.\n",
        "I also have a \"Boxer\" feature: This feature is set to \"true\" if the dog is a Boxer and \"false\" if it's not.\n",
        "So, if I have a Pitbull, I set \"Pitbull\" to \"true\" and \"Boxer\" to \"false,\" and vice versa for Boxer Dogs. This way, I represent the breed of the dog using these two binary features instead of a one-hot encoding with multiple columns.\n",
        "\n",
        "\n",
        "If you are confused by the implmentation its exactly like the color implmentation in the first project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4vX0pCCp67o"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train=X_train.dropna(subset=['AgeuponOutcome'])\n",
        "X_test=X_test.dropna(subset=['AgeuponOutcome'])\n",
        "\n",
        "\n",
        "\n",
        "def age_converter(row):\n",
        "    age_string = row['AgeuponOutcome']\n",
        "    [age,unit] = age_string.split(\" \")\n",
        "    unit = unit.lower()\n",
        "    if(\"day\" in unit):\n",
        "        if age=='0': return 1\n",
        "        return int(age)\n",
        "    if(\"week\" in unit):\n",
        "        if(age)=='0': return 7\n",
        "        return int(age)*7\n",
        "    elif(\"month\" in unit):\n",
        "        if(age)=='0': return 30\n",
        "        return int(age) *30\n",
        "    elif(\"year\" in unit):\n",
        "        if(age)=='0': return  365\n",
        "        return int(age) *365\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train['Age numeric'] = X_train.apply(age_converter, axis=1)\n",
        "X_test['Age numeric'] = X_test.apply(age_converter, axis=1)\n",
        "\n",
        "\n",
        "X_train = X_train.drop('AgeuponOutcome', axis=True)\n",
        "X_test = X_test.drop('AgeuponOutcome', axis=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWvV2_4D2r2t"
      },
      "source": [
        "age came as : (int: quantity ,string:unit): example(1,year)\n",
        "I transferred the unit into its daily presentation:{year : 365,month:30,week:7,day :1}\n",
        "so\n",
        "basically it’s transferring the unit into it’s daily presentation and then multiplying it by the quantity\n",
        "\n",
        "example: 1 year   =  1 * 365\n",
        "\n",
        "from the age we can see that sometimes there is (0,unit) example (0,years) therefore we can conclude that there might be a lot of problems that are not shown in the data set\n",
        "the problem have been fixed by if we have (0,unit) then (1*unit) example (0,years)=1**365\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys11XmTFsjJP"
      },
      "outputs": [],
      "source": [
        "counts = X_train.groupby('Age numeric')['OutcomeType'].value_counts(normalize=True).mul(100).unstack()\n",
        "\n",
        "counts.plot.barh(stacked=True, colormap='Set1', figsize=(10, 8))\n",
        "plt.xlabel('Percentage')\n",
        "plt.ylabel('Age')\n",
        "plt.title('The relation Between Age and OutCome')\n",
        "plt.tick_params(axis='y', which='major', labelsize=14, labelrotation=0, length=5)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HaYgEvJv_WD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def Categorizing_Age (DataFrame):\n",
        "  DataFrame['Child_Age']=DataFrame['Age numeric']<35\n",
        "  DataFrame['Young_Age']=(DataFrame['Age numeric']<365)&(DataFrame[\"Age numeric\"]>35)\n",
        "  DataFrame['Age_Year']=DataFrame['Age numeric']==365\n",
        "  DataFrame['Age is Greater']=DataFrame['Age numeric']>365\n",
        "\n",
        "Categorizing_Age(X_train)\n",
        "Categorizing_Age(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSEJOmAC3Ba_"
      },
      "source": [
        "I have categorized the Age into categories and this to consider and heighlight the Age diffrences between animals so the machine learning module can understand the differences between different categories\n",
        "\n",
        "by categorizing data based on age, the machine learning module is essentially identifying patterns and distinctions between the age categories rather than attempting to learn the specific differences between individual ages. This simplification allows the model to focus on broader trends and relationships within each category, leading to more efficient and effective predictions or classifications based on age-related data. This approach enhances the model's generalization capabilities, making it better equipped to handle new, unseen data, and it also facilitates the generation of more interpretable and actionable insights.\n",
        "\n",
        "By putting data into categories, we're showing which groups have a strong link to the outcome type. This makes it clear that the machine learning model is likely to predict non-adoption for ages between 1 and 35. and a better chance of adoption for age 60 and above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXnnDqMUcKsy"
      },
      "outputs": [],
      "source": [
        "y_train=X_train.pop(\"OutcomeType\")\n",
        "y_test=X_test.pop(\"OutcomeType\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nThxlIzMPMGC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import bisect\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "X_train[\"Breed_encoded\"] = label_encoder.fit_transform(X_train[\"Breed\"])\n",
        "\n",
        "label_classes = label_encoder.classes_.tolist()\n",
        "\n",
        "def Test_Breed_Encoder(row,label_classes):\n",
        "  breed=row[\"Breed\"]\n",
        "  if breed in label_classes:\n",
        "    return breed\n",
        "  else:\n",
        "    return 'Other'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_test[\"Breed\"] = X_test.apply(Test_Breed_Encoder, args=(label_classes,), axis=1)\n",
        "label_classes.append(\"Other\")\n",
        "\n",
        "label_encoder.classes_=np.array(label_classes)\n",
        "\n",
        "X_test[\"Breed_encoded\"] = label_encoder.transform(X_test[\"Breed\"])\n",
        "\n",
        "\n",
        "X_test=X_test.drop(\"Breed\",axis=True)\n",
        "X_train=X_train.drop(\"Breed\",axis=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmVF3ys9RXzu"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from imblearn.pipeline import Pipeline as imbpipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=3000)\n",
        "\n",
        "one_hot=OneHotEncoder(sparse_output=True,handle_unknown='ignore',max_categories=40)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "One_Hot_Encoding = make_column_transformer(\n",
        "    (one_hot, [\"Breed_encoded\"]),\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "pipe = imbpipeline([\n",
        "    ('preprocessor', One_Hot_Encoding),\n",
        "    ('scaler', StandardScaler()),\n",
        "    (('sample',SMOTE(random_state=42, k_neighbors=3))),\n",
        "    ('classifier', log_reg)\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "  \"classifier__class_weight\": [{0: 1, 1: 0.8}, {0: 1, 1: 0.85}, {0: 1, 1: 0.75}, {0: 1, 1: 0.83}],\n",
        "  'classifier__solver': ['saga', 'lbfgs'],\n",
        "  'classifier__penalty': ['l2', None]\n",
        "}\n",
        "grid_search = GridSearchCV(pipe, parameters, cv=2,scoring=\"f1\")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "Logistic_regression_best_parameters = grid_search.best_params_\n",
        "print(\"Best Parameters : \",Logistic_regression_best_parameters)\n",
        "\n",
        "Logistic_regression_best_moduel=grid_search.best_estimator_\n",
        "\n",
        "\n",
        "\n",
        "scores = cross_validate(Logistic_regression_best_moduel, X_train, y_train, return_train_score='f1_score',cv=5)\n",
        "test_accuracy=scores[\"test_score\"]\n",
        "train_accuracy=scores[\"train_score\"]\n",
        "\n",
        "plt.boxplot([test_accuracy, train_accuracy], labels=[\"test\", \"train\"], showmeans=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_u-XbsPzlHv"
      },
      "source": [
        "#Standard scaling\n",
        "is a helpful technique in machine learning because it transforms features into a common format, making them comparable. This normalization reduces bias that could arise when some features have larger values than others, ensuring fair treatment for all features.\n",
        "#Smote Function\n",
        "Smote is a tool that allow us to balance the classes inside the dataset and it works by:\n",
        "\n",
        "1) Identify Minority Class: Identify the minority class, the one with fewer instances in the dataset.\n",
        "\n",
        "2) Discover Common Feature Values: Find common feature values within the minority class, revealing its characteristic patterns.\n",
        "\n",
        "3) Generate Synthetic Samples: Create synthetic samples based on these common feature values to represent the minority class more effectively.\n",
        "\n",
        "4) Check Neighboring Samples: Ensure that the synthetic samples fit within the neighborhood of the minority class by comparing them to nearby instances.\n",
        "\n",
        "Balance Classes: By following these steps, you balance the class distribution, resulting in a more balanced dataset, which can enhance the performance of machine learning models, particularly in cases with class imbalance.\n",
        "\n",
        "\n",
        "#LBFG Solver\n",
        "LBFG Solver is a way to fasten the proccess of minmising the loss function\n",
        "\n",
        "How it works:\n",
        "1)It tries to minmise the loss function by computing both the First derivative and the second derivative\n",
        "\n",
        "It uses the approach that if we have a positive value in the first derivative and the second derivative have a positive value then it will try to minmise the learning rate because we are going to the direction where the loss function will get into 0 and if the first derivative is less than zero yet the function is decreasing then we will try to increase the learning rate in order to arrive to the root faster\n",
        "\n",
        "\n",
        "#F1 Score:\n",
        "F1 Score is a very standart thing to use in order to compute an implanced data set and this is becaise it combines both recall and precession  \n",
        "\n",
        "The true meaning of F1 is basically from the classes that we got how precise we were at identfying them\n",
        "\n",
        "And this means that we are taking the miniority class into our consderation this time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGkDcs-O7NjI"
      },
      "source": [
        "Logistic Regression is like a decision-making tool for yes-or-no questions. It uses a special math formula that takes information and gives you a number between 0 and 1.\n",
        "\n",
        "If the number is close to 1, it says \"Yes, it's a 'Yes' answer.\"\n",
        "If the number is close to 0, it says \"No, it's a 'No' answer.\"\n",
        "\n",
        "So, it helps us decide whether something belongs to one group or another based on the information we have. It's like asking a very smart friend for advice, and they give you a number that helps you make a clear decision.\n",
        "\n",
        "\n",
        "Most importantly it uses some loss function and after using the loss function it tries to minmise the loss function by using the  gradient decent method to compute better coefficents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vjv9wx3uWw8W"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "Logistic_regression_best_moduel.fit(X_train,y_train)\n",
        "y_hat = Logistic_regression_best_moduel.predict(X_test)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_hat)\n",
        "\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index =[\"NotAdoption\",\"Adoption\"],columns = [\"NotAdoption\",\"Adoption\"])\n",
        "plt.figure(figsize=(5,4))\n",
        "\n",
        "\n",
        "sns.heatmap(cm_df / len(X_test), annot=True)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1EjC1tl1W5t"
      },
      "outputs": [],
      "source": [
        "f1_Score=f1_score(y_test, y_hat)\n",
        "accuarcy_Score=accuracy_score(y_test, y_hat)\n",
        "print(\"F1 Score : \",f1_Score,\",\",\"Accuarcy Score : \",accuarcy_Score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QjvC9Usl8ou"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "One_Hot_Encoding = make_column_transformer(\n",
        "    (one_hot, [\"Breed_encoded\"]),\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "pipeline = imbpipeline([\n",
        "    ('preprocessor', One_Hot_Encoding),\n",
        "    ('scaler', StandardScaler()),\n",
        "    (('sample',SMOTE(random_state=42, k_neighbors=3))),\n",
        "    ('classifier', GradientBoostingClassifier(random_state=42,n_iter_no_change=5))])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "parameters = {\n",
        "    \"classifier__n_estimators\": [500,750,1000],\n",
        "    \"classifier__learning_rate\": [0.01, 0.05],\n",
        "    \"classifier__max_depth\": [5,10,15,20],\n",
        "    \"classifier__min_samples_split\": [4,3],\n",
        "    \"classifier__min_samples_leaf\": [3,4],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, parameters, cv=2,scoring=\"f1\",n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "Gradient_tree_best_parameters = grid_search.best_params_\n",
        "print(\"Best Parameters : \",Gradient_tree_best_parameters)\n",
        "Gradient_tree_best_module=grid_search.best_estimator_\n",
        "\n",
        "\n",
        "scores = cross_validate(Gradient_tree_best_module, X_train, y_train, return_train_score=\"f1_score\",cv=5)\n",
        "Gradient_tree_best_module.fit(X_train,y_train)\n",
        "test_accuarcy=scores[\"test_score\"]\n",
        "train_accuarcy=scores[\"train_score\"]\n",
        "\n",
        "\n",
        "\n",
        "plt.boxplot([test_accuarcy, train_accuarcy], labels=[\"test\", \"train\"], showmeans=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wCyX1mN-9uA"
      },
      "source": [
        "Gradient Boosting Classifier\n",
        "\n",
        "It works making simple leaf for example let's say a random leaf that always say either True Or False \"Basically a Constant\"\n",
        "Then it looks at the error that is coming from these predictions by using a certian loss function like the log loss\n",
        "\n",
        "then it tires to classify the classes that have not been correctly classified by moving toward the direction that reduces the loss function the most in the next tree\n",
        "\n",
        "By adding all the trees together we will obtain a tree that can classify things correctly\n",
        "\n",
        "\n",
        "Note:\n",
        "The choice of parameters haven't been at random and this is because i didnt want to tune the machine learning module manually\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6k3euKOyTN-"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H39eUNjNv6Vl"
      },
      "outputs": [],
      "source": [
        "Gradient_tree_best_module.fit(X_train,y_train)\n",
        "y_hat = Gradient_tree_best_module.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_hat)\n",
        "\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index =[\"NotAdoption\",\"Adoption\"],columns = [\"NotAdoption\",\"Adoption\"])\n",
        "plt.figure(figsize=(5,4))\n",
        "\n",
        "sns.heatmap(cm_df / len(X_test), annot=True)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_Wod-x8063t"
      },
      "outputs": [],
      "source": [
        "f1_Score=f1_score(y_test, y_hat)\n",
        "accuarcy_Score=accuracy_score(y_test, y_hat)\n",
        "print(\"F1 Score : \",f1_Score,\",\",\"Accuarcy Score : \",accuarcy_Score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCeRr7BO_u2g"
      },
      "source": [
        "Under We have a Confusion matrix that explains how much a feature is important in order to make a prediction for the machine learning module\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjFqKObECFKH"
      },
      "source": [
        "Random Forest uses a group of decision trees that analyze data independently. Each tree is trained on a random subset of data and features, which adds diversity. They might be a bit biased, but they all vote on the prediction. The majority vote leads to a more reliable and balanced final prediction, combining the strengths of individual trees\n",
        "\n",
        "Example:\n",
        "Imagine a scenario where a decision needs to be made about increasing military expenses. The general supports the idea\n",
        "Religious leaders oppose it\n",
        "The high-tech industry is also against it\n",
        "The economy shows resistance as well\n",
        "In this case, individuals from diverse backgrounds collaborate to reach a collective decision, which is determined by a majority vote. Therefore, people with different experiences come together to make a decision collectively, ultimately relying on a majority consensus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPGqJGTQwRk-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "pipeline = imbpipeline([\n",
        "    ('preprocessor', One_Hot_Encoding),\n",
        "    ('scaler', StandardScaler()),\n",
        "    (('sample',SMOTE(random_state=42, k_neighbors=3))),\n",
        "    ('classifier',RandomForestClassifier(random_state=42))])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "parameters = {\n",
        "    \"classifier__n_estimators\": [1250,1500],\n",
        "    \"classifier__max_depth\": [4,6,8,10],\n",
        "    \"classifier__min_samples_split\": [50, 100 ,200],\n",
        "    \"classifier__min_samples_leaf\": [3,4],\n",
        "    \"classifier__class_weight\": [{0: 1, 1: 0.86}, {0: 1, 1: 0.84}, {0:1, 1: 0.82}]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, parameters, cv=2, scoring=\"f1\",n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "Random_forest_best_parameters = grid_search.best_params_\n",
        "print(\"Best Parameters : \",Random_forest_best_parameters)\n",
        "Random_forest_best_moduel=grid_search.best_estimator_\n",
        "\n",
        "scores = cross_validate(Random_forest_best_moduel, X_train, y_train, return_train_score=\"f1_score\",cv=5)\n",
        "test_accuarcy=scores[\"test_score\"]\n",
        "train_accuarcy=scores[\"train_score\"]\n",
        "\n",
        "\n",
        "\n",
        "plt.boxplot([test_accuarcy, train_accuarcy], labels=[\"test\", \"train\"], showmeans=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USAJ1i83EHWq"
      },
      "outputs": [],
      "source": [
        "y_hat = Random_forest_best_moduel.predict(X_test)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_hat)\n",
        "\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index =[\"NotAdoption\",\"Adoption\"],columns = [\"NotAdoption\",\"Adoption\"])\n",
        "plt.figure(figsize=(5,4))\n",
        "\n",
        "sns.heatmap(cm_df / len(X_test), annot=True)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAHhdS-509RN"
      },
      "outputs": [],
      "source": [
        "f1_Score=f1_score(y_test, y_hat)\n",
        "accuarcy_Score=accuracy_score(y_test, y_hat)\n",
        "print(\"F1 Score : \",f1_Score,\",\",\"Accuarcy Score : \",accuarcy_Score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3eiq06D0JvY"
      },
      "source": [
        "kneighborsclassifier its looking at features that our sample have and it tryies to identify it by looking at the other samples and which feature values they have\n",
        "\n",
        "Using a certain metric we try to know how far the samples are from the our sample that we are trying to identify\n",
        "\n",
        "in order to set a good estimate in how similar our sample to the other samples we can look how the other samples are far away from it and this what weight does there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7InH6Lr9NvM0"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "\n",
        "One_Hot_Encoding = make_column_transformer(\n",
        "    (one_hot, [\"Breed_encoded\"]),\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "pipeline = imbpipeline([\n",
        "    ('preprocessor', One_Hot_Encoding),\n",
        "    ('scaler', StandardScaler()),\n",
        "    (('sample',SMOTE(random_state=42, k_neighbors=3))),\n",
        "    ('classifier', KNeighborsClassifier())])\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'classifier__n_neighbors': [7,9,12,15],\n",
        "    'classifier__weights': ['uniform', 'distance'],\n",
        "    'classifier__metric': ['euclidean', 'manhattan'],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=2,scoring=\"f1\",n_jobs=-1)\n",
        "\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "K_nearest_neighboor_best_parameters = grid_search.best_params_\n",
        "\n",
        "print(\"Best Parameters : \",K_nearest_neighboor_best_parameters)\n",
        "K_nearest_neighboor_best_module=grid_search.best_estimator_\n",
        "\n",
        "\n",
        "scores = cross_validate(K_nearest_neighboor_best_module, X_train, y_train, return_train_score=\"f1_score\",cv=5)\n",
        "\n",
        "test_accuarcy=scores[\"test_score\"]\n",
        "train_accuarcy=scores[\"train_score\"]\n",
        "\n",
        "\n",
        "\n",
        "plt.boxplot([test_accuarcy, train_accuarcy], labels=[\"test\", \"train\"], showmeans=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbVH_F0uOGQl"
      },
      "outputs": [],
      "source": [
        "K_nearest_neighboor_best_module.fit(X_train,y_train)\n",
        "y_hat = K_nearest_neighboor_best_module.predict(X_test)\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_hat)\n",
        "\n",
        "cm_df = pd.DataFrame(cm,\n",
        "                     index =[\"NotAdoption\",\"Adoption\"],columns = [\"NotAdoption\",\"Adoption\"])\n",
        "plt.figure(figsize=(5,4))\n",
        "\n",
        "sns.heatmap(cm_df / len(X_test), annot=True)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual Values')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PiQ3wKCptFC"
      },
      "outputs": [],
      "source": [
        "f1_Score=f1_score(y_test, y_hat)\n",
        "accuarcy_Score=accuracy_score(y_test, y_hat)\n",
        "print(\"F1 Score : \",f1_Score,\",\",\"Accuarcy Score : \",accuarcy_Score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0vadHF2Bbl2"
      },
      "source": [
        "Here i have been asked by the teacher to make an example where the machine learning module dosn't use the Feature Age since it's an important feature\n",
        "\n",
        "Therefore i have made a simple Function in order to drop the Age Feature and the Feature engneering that have provided one of the most important features Age Between 35 - 365\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtgbCmK2oZgF"
      },
      "outputs": [],
      "source": [
        "\n",
        "Index_List=list()\n",
        "Feature_Names=X_train.columns\n",
        "for Feature_name in Feature_Names:\n",
        "  Feature_name=Feature_name.split(' ')\n",
        "  if \"Age\" in Feature_name:\n",
        "    Index_List.append(Counter)\n",
        "\n",
        "  Counter +=1\n",
        "\n",
        "\n",
        "def Age_Feature_Remove(DataFrame,Feature_Index):\n",
        "  DataFrame=DataFrame.drop(DataFrame.columns[Feature_Index],axis=1)\n",
        "  return DataFrame\n",
        "X_train=Age_Feature_Remove(X_train,Index_List)\n",
        "\n",
        "X_test=Age_Feature_Remove(X_test,Index_List)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtPnfG4ezezZ"
      },
      "source": [
        "#K Nearest Neighboor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZ2qVQpdblI0"
      },
      "outputs": [],
      "source": [
        "pipeline = imbpipeline([\n",
        "    ('preprocessor', One_Hot_Encoding),\n",
        "    ('scaler', StandardScaler()),\n",
        "    (('sample',SMOTE(random_state=42, k_neighbors=3))),\n",
        "    ('classifier', KNeighborsClassifier(metric='manhattan', n_neighbors= 7, weights= 'distance'))])\n",
        "\n",
        "\n",
        "scores = cross_validate(pipeline, X_train, y_train, return_train_score=\"f1_score\",cv=5)\n",
        "\n",
        "test_accuarcy=scores[\"test_score\"]\n",
        "train_accuarcy=scores[\"train_score\"]\n",
        "\n",
        "\n",
        "\n",
        "plt.boxplot([test_accuarcy, train_accuarcy], labels=[\"test\", \"train\"], showmeans=True)\n",
        "plt.show()\n",
        "\n",
        "pipeline.fit(X_train,y_train)\n",
        "y_hat=pipeline.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-lIf82N0sy1"
      },
      "outputs": [],
      "source": [
        "f1_Score=f1_score(y_test, y_hat)\n",
        "accuarcy_Score=accuracy_score(y_test, y_hat)\n",
        "print(\"F1 Score : \",f1_Score,\",\",\"Accuarcy Score : \",accuarcy_Score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGbZui94zkVc"
      },
      "source": [
        "#Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynfWOGSwbyMo"
      },
      "outputs": [],
      "source": [
        "pipeline = imbpipeline([\n",
        "    ('preprocessor', One_Hot_Encoding),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('sample', SMOTE(random_state=42, k_neighbors=3)),\n",
        "    ('classifier', RandomForestClassifier(class_weight={0: 1, 1: 0.86}, max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=1500))\n",
        "])\n",
        "\n",
        "scores = cross_validate(pipeline, X_train, y_train, return_train_score=\"f1_score\",cv=5)\n",
        "\n",
        "test_accuarcy=scores[\"test_score\"]\n",
        "train_accuarcy=scores[\"train_score\"]\n",
        "\n",
        "\n",
        "\n",
        "plt.boxplot([test_accuarcy, train_accuarcy], labels=[\"test\", \"train\"], showmeans=True)\n",
        "plt.show()\n",
        "\n",
        "pipeline.fit(X_train,y_train)\n",
        "y_hat=pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY_vIYBp0mO_"
      },
      "outputs": [],
      "source": [
        "f1_Score=f1_score(y_test, y_hat)\n",
        "accuarcy_Score=accuracy_score(y_test, y_hat)\n",
        "print(\"F1 Score : \",f1_Score,\",\",\"Accuarcy Score : \",accuarcy_Score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5jHgO1fzn7q"
      },
      "source": [
        "Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RldfZmsb1Vo"
      },
      "outputs": [],
      "source": [
        "\n",
        "pipeline = imbpipeline([\n",
        "    ('preprocessor', One_Hot_Encoding),\n",
        "    ('scaler', StandardScaler()),\n",
        "    (('sample',SMOTE(random_state=42, k_neighbors=3))),\n",
        "    ('classifier', GradientBoostingClassifier(random_state=42,n_iter_no_change=5,learning_rate= 0.01,max_depth= 5, min_samples_leaf= 4, min_samples_split= 4,n_estimators= 750))])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "scores = cross_validate(pipeline, X_train, y_train, return_train_score=\"f1_score\",cv=5)\n",
        "\n",
        "test_accuarcy=scores[\"test_score\"]\n",
        "train_accuarcy=scores[\"train_score\"]\n",
        "\n",
        "\n",
        "\n",
        "plt.boxplot([test_accuarcy, train_accuarcy], labels=[\"test\", \"train\"], showmeans=True)\n",
        "plt.show()\n",
        "\n",
        "pipeline.fit(X_train,y_train)\n",
        "y_hat=pipeline.predict(X_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3lvlZMP0oYt"
      },
      "outputs": [],
      "source": [
        "f1_Score=f1_score(y_test, y_hat)\n",
        "accuarcy_Score=accuracy_score(y_test, y_hat)\n",
        "print(\"F1 Score : \",f1_Score,\",\",\"Accuarcy Score : \",accuarcy_Score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRkNYs0hzrts"
      },
      "source": [
        "#Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mClzQLGwb5Ru"
      },
      "outputs": [],
      "source": [
        "pipeline = imbpipeline([\n",
        "    ('preprocessor', One_Hot_Encoding),\n",
        "    ('scaler', StandardScaler()),\n",
        "    (('sample',SMOTE(random_state=42, k_neighbors=3))),\n",
        "    ('log_reg', LogisticRegression(max_iter=3000,class_weight= {0: 1, 1: 0.75}, penalty= 'l2', solver= 'saga'))\n",
        "])\n",
        "\n",
        "\n",
        "scores = cross_validate(pipeline, X_train, y_train, return_train_score=\"f1_score\",cv=5)\n",
        "\n",
        "test_accuarcy=scores[\"test_score\"]\n",
        "train_accuarcy=scores[\"train_score\"]\n",
        "\n",
        "\n",
        "\n",
        "plt.boxplot([test_accuarcy, train_accuarcy], labels=[\"test\", \"train\"], showmeans=True)\n",
        "plt.show()\n",
        "\n",
        "pipeline.fit(X_train,y_train)\n",
        "y_hat=pipeline.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHCzyF3R0pFs"
      },
      "outputs": [],
      "source": [
        "f1_Score=f1_score(y_test, y_hat)\n",
        "accuarcy_Score=accuracy_score(y_test, y_hat)\n",
        "print(\"F1 Score : \",f1_Score,\",\",\"Accuarcy Score : \",accuarcy_Score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}